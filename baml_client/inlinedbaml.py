###############################################################################
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml-py
#
###############################################################################

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code.
#
# ruff: noqa: E501,F401
# flake8: noqa: E501,F401
# pylint: disable=unused-import,line-too-long
# fmt: off

file_map = {
    
    "agent.baml": "// Define an agent that can process user queries\nclass AgentInput {\n  query string\n}\n\nclass AgentOutput {\n  response string\n  action string?\n}\n\nfunction Agent(input: AgentInput) -> AgentOutput {\n  client CustomGPT4oMini\n  prompt #\"\n    \n    {{ _.role(\"system\") }}     \n    You are a helpful assistant that can answer questions about resumes.\n\n    {{ _.role(\"user\")}} \n    \n    User query: {{ input.query }}\n    \n    Respond to the user's query. If the query is about extracting information from a resume,\n    suggest using the ExtractResume function and set the action field to \"extract_resume\".\n    \n    For example, if the user asks \"Can you help me extract data from my resume?\", your response\n    should be helpful and you should set action to \"extract_resume\".\n    \n    If the query is not about resume extraction, provide a helpful response and leave the action field empty.\n    \n    {{ ctx.output_format }}\n  \"#\n}\n\n// Test the agent with a sample query\ntest agent_test {\n  functions [Agent]\n  args {\n    input {\n      query \"Can you help me extract information from a resume?\"\n    }\n  }\n}\n\n// Test the agent with a non-resume query\ntest agent_general_query {\n  functions [Agent]\n  args {\n    input {\n      query \"What is BAML?\"\n    }\n  }\n} ",
    "agent_with tools1.baml": "// Define an agent planner that generates a sequence of tool calls based on a paragraph\nclass AgentPlannerInput {\n  paragraph string\n  available_tools string[]\n}\n\nclass AgentPlannerOutput {\n  plan_description string\n  tool_calls ToolCall[]\n}\n\nclass ToolCall {\n  tool_name string\n  parameters string  // JSON string of parameters\n  purpose string     // Brief explanation of why this tool is being called\n}\n\nfunction AgentPlanner(input: AgentPlannerInput) -> AgentPlannerOutput {\n  client CustomGPT4oMini\n  prompt #\"\n    You are an AI planning assistant that creates a sequence of tool calls to accomplish tasks described in a paragraph.\n    \n    Available tools:\n    {% for tool in input.available_tools %}\n    - {{ tool }}\n    {% endfor %}\n    \n    Input paragraph: {{ input.paragraph }}\n    \n    Your task is to analyze the paragraph and create a plan with a sequence of tool calls that would be needed to accomplish the tasks described.\n    \n    For each tool call, specify:\n    1. The tool name (must be one of the available tools)\n    2. The parameters as a JSON string\n    3. A brief explanation of why this tool is being called\n    \n    For example, if the paragraph is \"I need to check the weather in New York and then search for information about climate change\", your response might be:\n    {\n      \"plan_description\": \"First check the weather in New York, then search for information about climate change.\",\n      \"tool_calls\": [\n        {\n          \"tool_name\": \"get_weather\",\n          \"parameters\": \"{\\\\\"location\\\\\": \\\\\"New York\\\\\"}\",\n          \"purpose\": \"To check the current weather in New York\"\n        },\n        {\n          \"tool_name\": \"search\",\n          \"parameters\": \"{\\\\\"query\\\\\": \\\\\"information about climate change\\\\\"}\",\n          \"purpose\": \"To find information about climate change\"\n        }\n      ]\n    }\n    \n    If no tool calls are needed or the available tools don't match the needs in the paragraph, return an empty array for tool_calls.\n    \n    {{ ctx.output_format }}\n  \"#\n}\n\n// Test the planner with a paragraph requiring multiple tools\ntest planner_multiple_tools {\n  functions [AgentPlanner]\n  args {\n    input {\n      paragraph \"I need to check the weather in San Francisco and then extract information from my resume.\"\n      available_tools [\"get_weather\", \"search\", \"extract_resume\", \"duckduckgo_search\"]\n    }\n  }\n}\n\n// Test the planner with a paragraph requiring a single tool\ntest planner_single_tool {\n  functions [AgentPlanner]\n  args {\n    input {\n      paragraph \"I need to search for information about machine learning algorithms.\"\n      available_tools [\"get_weather\", \"search\", \"extract_resume\", \"duckduckgo_search\"]\n    }\n  }\n}\n\n// Test the planner with a paragraph that doesn't require tools\ntest planner_no_tools {\n  functions [AgentPlanner]\n  args {\n    input {\n      paragraph \"Tell me about your capabilities as an AI assistant.\"\n      available_tools [\"sum\", \"subtract\", \"multiply\", \"divide\"]\n    }\n  }\n}",
    "agent_with_tools.baml": "// Define an enhanced agent that can use tools\nclass AgentWithToolsInput {\n  query string\n  available_tools string[]\n}\n\nclass AgentWithToolsOutput {\n  response string\n  tool_to_use string?\n  tool_parameters string?\n}\n\nfunction AgentWithTools(input: AgentWithToolsInput) -> AgentWithToolsOutput {\n  client CustomGPT4oMini\n  prompt #\"\n    You are a helpful assistant that can use tools to accomplish tasks.\n    \n    Available tools:\n    {% for tool in input.available_tools %}\n    - {{ tool }}\n    {% endfor %}\n    \n    User query: {{ input.query }}\n    \n    Respond to the user's query. If you need to use a tool to answer the query, specify which tool to use\n    in the tool_to_use field and provide any necessary parameters in the tool_parameters field as a JSON string.\n    \n    For example, if the user asks about the weather in New York, you might respond:\n    {\n      \"response\": \"I'll check the current weather in New York for you.\",\n      \"tool_to_use\": \"get_weather\",\n      \"tool_parameters\": \"{\\\\\"location\\\\\": \\\\\"New York\\\\\"}\"\n    }\n    \n    If the user asks about extracting information from a resume, you might respond:\n    {\n      \"response\": \"I can help extract information from your resume. Please provide the resume text.\",\n      \"tool_to_use\": \"extract_resume\",\n      \"tool_parameters\": null\n    }\n    \n    If you don't need to use a tool, leave the tool_to_use and tool_parameters fields empty.\n    \n    {{ ctx.output_format }}\n  \"#\n}\n\n// Test the agent with a weather query\ntest agent_weather_query {\n  functions [AgentWithTools]\n  args {\n    input {\n      query \"What's the weather like in San Francisco?\"\n      available_tools [\"get_weather\", \"search\", \"extract_resume\", \"duckduckgo_search\"]\n    }\n  }\n}\n\n// Test the agent with a resume query\ntest agent_resume_query {\n  functions [AgentWithTools]\n  args {\n    input {\n      query \"Can you help me extract information from my resume?\"\n      available_tools [\"get_weather\", \"search\", \"extract_resume\", \"duckduckgo_search\"]\n    }\n  }\n}\n\n// Test the agent with a general query\ntest agent_general_query {\n  functions [AgentWithTools]\n  args {\n    input {\n      query \"What is BAML?\"\n      available_tools [\"get_weather\", \"search\", \"extract_resume\", \"duckduckgo_search\"]\n    }\n  }\n}\n\n// Test the agent with a DuckDuckGo search query\ntest agent_duckduckgo_search_query {\n  functions [AgentWithTools]\n  args {\n    input {\n      query \"What are the latest developments in AI?\"\n      available_tools [\"get_weather\", \"search\", \"extract_resume\", \"duckduckgo_search\"]\n    }\n  }\n} ",
    "clients.baml": "// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\n\nclient<llm> CustomGPT4o {\n  provider openai\n  options {\n    model \"gpt-4o\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomGPT4oMini {\n  provider openai\n  retry_policy Exponential\n  options {\n    model \"gpt-4o-mini\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomSonnet {\n  provider anthropic\n  options {\n    model \"claude-3-5-sonnet-20241022\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n\nclient<llm> CustomHaiku {\n  provider anthropic\n  retry_policy Constant\n  options {\n    model \"claude-3-haiku-20240307\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/round-robin\nclient<llm> CustomFast {\n  provider round-robin\n  options {\n    // This will alternate between the two clients\n    strategy [CustomGPT4oMini, CustomHaiku]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/fallback\nclient<llm> OpenaiFallback {\n  provider fallback\n  options {\n    // This will try the clients in order until one succeeds\n    strategy [CustomGPT4oMini, CustomGPT4oMini]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/retry\nretry_policy Constant {\n  max_retries 3\n  // Strategy is optional\n  strategy {\n    type constant_delay\n    delay_ms 200\n  }\n}\n\nretry_policy Exponential {\n  max_retries 2\n  // Strategy is optional\n  strategy {\n    type exponential_backoff\n    delay_ms 300\n    multiplier 1.5\n    max_delay_ms 10000\n  }\n}",
    "generators.baml": "// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"python/pydantic\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.89.0\"\n\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode sync\n}\n",
    "resume.baml": "// Defining a data model.\nclass Resume {\n  name string\n  email string\n  experience string[]\n  skills string[]\n}\n\n// Create a function to extract the resume from a string.\nfunction ExtractResume(resume: string) -> Resume {\n  // Specify a client as provider/model-name\n  // you can use custom LLM params with a custom client name from clients.baml like \"client CustomHaiku\"\n  client \"openai/gpt-4o\" // Set OPENAI_API_KEY to use this client.\n  prompt #\"\n    Extract from this content:\n    {{ resume }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n\n\n\n// Test the function with a sample resume. Open the VSCode playground to run this.\ntest vaibhav_resume {\n  functions [ExtractResume]\n  args {\n    resume #\"\n      Vaibhav Gupta\n      vbv@boundaryml.com\n\n      Experience:\n      - Founder at BoundaryML\n      - CV Engineer at Google\n      - CV Engineer at Microsoft\n\n      Skills:\n      - Rust\n      - C++\n    \"#\n  }\n}\n",
}

def get_baml_files():
    return file_map