###############################################################################
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml-py
#
###############################################################################

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code.
#
# ruff: noqa: E501,F401
# flake8: noqa: E501,F401
# pylint: disable=unused-import,line-too-long
# fmt: off

file_map = {
    
    "agent.baml": "// Define an agent that can process user queries\nclass AgentInput {\n  query string\n}\n\nclass AgentOutput {\n  response string\n  action string?\n}\n\nfunction Agent(input: AgentInput) -> AgentOutput {\n  client CustomGPT4oMini\n  prompt #\"\n    \n    {{ _.role(\"system\") }}     \n    You are a helpful assistant that can answer questions about resumes.\n\n    {{ _.role(\"user\")}} \n    \n    User query: {{ input.query }}\n    \n    Respond to the user's query. If the query is about extracting information from a resume,\n    suggest using the ExtractResume function and set the action field to \"extract_resume\".\n    \n    For example, if the user asks \"Can you help me extract data from my resume?\", your response\n    should be helpful and you should set action to \"extract_resume\".\n    \n    If the query is not about resume extraction, provide a helpful response and leave the action field empty.\n    \n    {{ ctx.output_format }}\n  \"#\n}\n\n// Test the agent with a sample query\ntest agent_test {\n  functions [Agent]\n  args {\n    input {\n      query \"Can you help me extract information from a resume?\"\n    }\n  }\n}\n\n// Test the agent with a non-resume query\ntest agent_general_query {\n  functions [Agent]\n  args {\n    input {\n      query \"What is BAML?\"\n    }\n  }\n} ",
    "agent_with_tools.baml": "// Define an enhanced agent that can use tools\nclass AgentWithToolsInput {\n  query string\n  available_tools string[]\n}\n\nclass AgentWithToolsOutput {\n  response string\n  tool_to_use string?\n  tool_parameters string?\n}\n\nfunction AgentWithTools(input: AgentWithToolsInput) -> AgentWithToolsOutput {\n  client CustomGPT4oMini\n  prompt #\"\n    You are a helpful assistant that can use tools to accomplish tasks.\n    \n    Available tools:\n    {% for tool in input.available_tools %}\n    - {{ tool }}\n    {% endfor %}\n    \n    User query: {{ input.query }}\n    \n    Respond to the user's query. If you need to use a tool to answer the query, specify which tool to use\n    in the tool_to_use field and provide any necessary parameters in the tool_parameters field as a JSON string.\n    \n    For example, if the user asks about the weather in New York, you might respond:\n    {\n      \"response\": \"I'll check the current weather in New York for you.\",\n      \"tool_to_use\": \"get_weather\",\n      \"tool_parameters\": \"{\\\\\"location\\\\\": \\\\\"New York\\\\\"}\"\n    }\n    \n    If the user asks about extracting information from a resume, you might respond:\n    {\n      \"response\": \"I can help extract information from your resume. Please provide the resume text.\",\n      \"tool_to_use\": \"extract_resume\",\n      \"tool_parameters\": null\n    }\n    \n    If you don't need to use a tool, leave the tool_to_use and tool_parameters fields empty.\n    \n    {{ ctx.output_format }}\n  \"#\n}\n\n// Test the agent with a weather query\ntest agent_weather_query {\n  functions [AgentWithTools]\n  args {\n    input {\n      query \"What's the weather like in San Francisco?\"\n      available_tools [\"get_weather\", \"search\", \"extract_resume\", \"duckduckgo_search\"]\n    }\n  }\n}\n\n// Test the agent with a resume query\ntest agent_resume_query {\n  functions [AgentWithTools]\n  args {\n    input {\n      query \"Can you help me extract information from my resume?\"\n      available_tools [\"get_weather\", \"search\", \"extract_resume\", \"duckduckgo_search\"]\n    }\n  }\n}\n\n// Test the agent with a general query\ntest agent_general_query {\n  functions [AgentWithTools]\n  args {\n    input {\n      query \"What is BAML?\"\n      available_tools [\"get_weather\", \"search\", \"extract_resume\", \"duckduckgo_search\"]\n    }\n  }\n}\n\n// Test the agent with a DuckDuckGo search query\ntest agent_duckduckgo_search_query {\n  functions [AgentWithTools]\n  args {\n    input {\n      query \"What are the latest developments in AI?\"\n      available_tools [\"get_weather\", \"search\", \"extract_resume\", \"duckduckgo_search\"]\n    }\n  }\n} ",
    "clients.baml": "// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\n\nclient<llm> CustomGPT4o {\n  provider openai\n  options {\n    model \"gpt-4o\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomGPT4oMini {\n  provider openai\n  retry_policy Exponential\n  options {\n    model \"gpt-4o-mini\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomSonnet {\n  provider anthropic\n  options {\n    model \"claude-3-5-sonnet-20241022\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n\nclient<llm> CustomHaiku {\n  provider anthropic\n  retry_policy Constant\n  options {\n    model \"claude-3-haiku-20240307\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/round-robin\nclient<llm> CustomFast {\n  provider round-robin\n  options {\n    // This will alternate between the two clients\n    strategy [CustomGPT4oMini, CustomHaiku]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/fallback\nclient<llm> OpenaiFallback {\n  provider fallback\n  options {\n    // This will try the clients in order until one succeeds\n    strategy [CustomGPT4oMini, CustomGPT4oMini]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/retry\nretry_policy Constant {\n  max_retries 3\n  // Strategy is optional\n  strategy {\n    type constant_delay\n    delay_ms 200\n  }\n}\n\nretry_policy Exponential {\n  max_retries 2\n  // Strategy is optional\n  strategy {\n    type exponential_backoff\n    delay_ms 300\n    multiplier 1.5\n    max_delay_ms 10000\n  }\n}",
    "generators.baml": "// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"python/pydantic\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.89.0\"\n\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode sync\n}\n",
    "resume.baml": "// Defining a data model.\nclass Resume {\n  name string\n  email string\n  experience string[]\n  skills string[]\n}\n\n// Create a function to extract the resume from a string.\nfunction ExtractResume(resume: string) -> Resume {\n  // Specify a client as provider/model-name\n  // you can use custom LLM params with a custom client name from clients.baml like \"client CustomHaiku\"\n  client \"openai/gpt-4o\" // Set OPENAI_API_KEY to use this client.\n  prompt #\"\n    Extract from this content:\n    {{ resume }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n\n\n\n// Test the function with a sample resume. Open the VSCode playground to run this.\ntest vaibhav_resume {\n  functions [ExtractResume]\n  args {\n    resume #\"\n      Vaibhav Gupta\n      vbv@boundaryml.com\n\n      Experience:\n      - Founder at BoundaryML\n      - CV Engineer at Google\n      - CV Engineer at Microsoft\n\n      Skills:\n      - Rust\n      - C++\n    \"#\n  }\n}\n",
}

def get_baml_files():
    return file_map